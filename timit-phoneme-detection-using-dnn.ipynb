{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#-- required imports\nimport tensorflow as tf\nfrom tensorflow.keras import regularizers as rg\nimport librosa\nfrom librosa import display\nfrom scipy.io import wavfile\nimport gc\nimport pickle as pkl\nfrom tqdm.notebook import tqdm, trange\nimport matplotlib.pyplot as plt\n\npath = '/kaggle/input/darpa-timit-acousticphonetic-continuous-speech'\ndata_path = path+\"/data\"","metadata":{"_uuid":"58c27144-cc7e-49e3-af1e-06340b6fb8a3","_cell_guid":"088cfc22-c1a7-41a7-a758-489471172eaf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-12T23:14:58.544789Z","iopub.execute_input":"2021-12-12T23:14:58.545270Z","iopub.status.idle":"2021-12-12T23:15:06.357117Z","shell.execute_reply.started":"2021-12-12T23:14:58.545225Z","shell.execute_reply":"2021-12-12T23:15:06.356213Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2021-12-12 23:14:59.160625: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2021-12-12 23:14:59.160744: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","output_type":"stream"}]},{"cell_type":"code","source":"class Callback(tf.keras.callbacks.Callback):\n    def on_epoch_begin(self,epoch,logs={}):\n        print(\"Epoch \",epoch)\n\n    def on_epoch_end(self,epoch,logs={}):\n        print('loss: {:.2f}, accuracy:{:.2f}'.format(\n                logs[\"loss\"],logs[\"accuracy\"]*100))\n        print(logs)\n        gc.collect()\n\n    def on_batch_end(self,batch,logs={}):\n        if(batch%100 == 0):\n            print(batch,'loss: {:.2f}, accuracy:{:.2f}'.format(\n                logs[\"loss\"],logs[\"accuracy\"]*100))\n\n    def on_test_batch_end(self, batch, logs=None):        \n        if(batch%100 == 0):\n            pass\n            return","metadata":{"execution":{"iopub.status.busy":"2021-12-12T23:15:06.359001Z","iopub.execute_input":"2021-12-12T23:15:06.359417Z","iopub.status.idle":"2021-12-12T23:15:06.367235Z","shell.execute_reply.started":"2021-12-12T23:15:06.359372Z","shell.execute_reply":"2021-12-12T23:15:06.366585Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import math\n\nclass DNN_MODULE_BUILDER():\n    __train_desc = 'train_data.csv'\n    __test_desc = 'test_data.csv'\n    __data_directory = './data'\n    __main_directory = './'\n    f_Path = 'path_from_data_dir' #field that contains file path in train_data.csv\n    f_IsAudio = 'is_converted_audio' #boolean field that tells that the record in train_data.csv contains the description of audio file we are interested in\n    f_IsPhon = 'is_phonetic_file'\n    # f_filename = 'filename' #field that contains filename\n    f_dr = 'dialect_region' #field that contains dialect_region information\n    _winlen = 0.025\n    _winstep = 0.01\n    \n    \n    def __init__(self,path=None):\n        self.__main_directory = path\n        if path[len(path)-1] == '/':\n            self.__data_directory = path+\"data/\"\n        else:\n            self.__main_directory += \"/\"\n            self.__data_directory = self.__main_directory+\"data/\"\n      \n        self.phon61_map39 = {\n            'iy':'iy',  'ih':'ih',   'eh':'eh',  'ae':'ae',    'ix':'ih',  'ax':'ah',   'ah':'ah',  'uw':'uw',\n            'ux':'uw',  'uh':'uh',   'ao':'aa',  'aa':'aa',    'ey':'ey',  'ay':'ay',   'oy':'oy',  'aw':'aw',\n            'ow':'ow',  'l':'l',     'el':'l',  'r':'r',      'y':'y',    'w':'w',     'er':'er',  'axr':'er',\n            'm':'m',    'em':'m',     'n':'n',    'nx':'n',     'en':'n',  'ng':'ng',   'eng':'ng', 'ch':'ch',\n            'jh':'jh',  'dh':'dh',   'b':'b',    'd':'d',      'dx':'dx',  'g':'g',     'p':'p',    't':'t',\n            'k':'k',    'z':'z',     'zh':'sh',  'v':'v',      'f':'f',    'th':'th',   's':'s',    'sh':'sh',\n            'hh':'hh',  'hv':'hh',   'pcl':'h#', 'tcl':'h#', 'kcl':'h#', 'qcl':'h#','bcl':'h#','dcl':'h#',\n            'gcl':'h#','h#':'h#',  '#h':'h#',  'pau':'h#', 'epi': 'h#','nx':'n',   'ax-h':'ah','q':'h#' \n        }\n        \n        self.phon61 = list(self.phon61_map39.keys())\n        self.phon39 = list(set(self.phon61_map39.values()))\n\n        self.label_p39 = {}\n        self.p39_label = {}\n        for i,p in enumerate(self.phon39):\n            self.label_p39[p] = i+1\n            self.p39_label[i+1] = p\n\n        self.phon39_map61 = {}\n        for p61,p39 in self.phon61_map39.items():\n            if not p39 in self.phon39_map61:\n                self.phon39_map61[p39] = []\n            self.phon39_map61[p39].append(p61)\n            \n    \n    def get39EquiOf61(self,p):\n        return self.phon61_map39[self.removePhonStressMarker(p)]\n\n    def removePhonStressMarker(self,phon):\n        phon = phon.replace('1','')\n        phon = phon.replace('2','')\n        return phon\n    \n    def getWindow(self,sr):\n        nfft = 512\n        winlen = self._winlen * sr\n        winstep = self._winstep * sr\n        return nfft,int(winlen),int(winstep)\n\n    def singleTrainingFrameSize(self,sr):\n        return math.floor(sr/4)\n        \n    def readTrainingDataDescriptionCSV(self):\n        file_path = self.__main_directory + 'train_data.csv' #check if train_data.csv is in correct path\n        self._Tdd = pd.read_csv(file_path)\n        # removing NaN entries in the train_data.csv file\n        dr = ['DR1','DR2','DR3','DR4','DR5','DR6','DR7','DR8']\n        self._Tdd = self._Tdd[self._Tdd['dialect_region'].isin(dr)]\n        return self._Tdd\n\n    def readTestingDataDescriptionCSV(self):\n        file_path = self.__main_directory + 'test_data.csv' #check if train_data.csv is in correct path\n        self._tdd = pd.read_csv(file_path)\n        # removing NaN entries in the train_data.csv file\n        dr = ['DR1','DR2','DR3','DR4','DR5','DR6','DR7','DR8']\n        self._tdd = self._tdd[self._tdd['dialect_region'].isin(dr)]\n        return self._tdd\n    \n    def getListAudioFiles(self,of='Train'):\n        if of == 'Train':\n            self.readTrainingDataDescriptionCSV()\n            return self._Tdd[self._Tdd[self.f_IsAudio] == True]\n        if of == 'Test':\n            self.readTestingDataDescriptionCSV()\n            return self._tdd[self._tdd[self.f_IsAudio] == True]\n        \n    def getListPhonemeFiles(self,of='Train'):\n        if of == 'Train':\n            self.readTrainingDataDescriptionCSV()\n            return self._Tdd[self._Tdd[self.f_IsPhon] == True]\n        if of == 'Test':\n            self.readTestingDataDescriptionCSV()\n            return self._tdd[self._tdd[self.f_IsPhon] == True]\n               \n    def readAudio(self,fpath=None,pre_emp = False):\n        if(fpath == None):\n            return np.zeros(1),0\n        \n        fpath = self.__data_directory+fpath\n        if os.path.exists(fpath):\n            S,sr = librosa.load(fpath,sr=None)\n            if pre_emp:\n                S = librosa.effects.preemphasis(S)\n            return S,sr   \n        else:\n            return np.zeros(1),0\n    \n    \n    def readPhon(self,fpath=None):\n        if(fpath == None):\n            raise Exception('phon file path not provided')\n        \n        fpath = self.__data_directory+fpath\n        ph_ = pd.read_csv(fpath,sep=\" \")#,usecols=['start','end','phoneme'])\n        #ph_.columns = ['start','end','phoneme']\n        return ph_\n            \n        pfn = j['filename'].split('.WAV')[0]+'.PHN'\n        p_bar.set_description(f'Working on {j[\"filename\"]} ,index: {c}  ')\n        try:\n            pfp = file_path+pfd[(pfd['filename']==pfn) & (pfd['speaker_id'] == j['speaker_id'])][f_Path].values[0]\n        except:\n            pfp = afp.replace(j['filename'],pfn)\n            \n        ph_ = pd.read_csv(pfp,sep=\" \")#,usecols=['start','end','phoneme'])\n        #ph_.columns = ['start','end','phoneme']\n    #---------------end readPhon()\n        \n    def getFeatureAndLabel(self,ftype='mfcc',audio_path=None,phon_path=None,n_mels=128,delta=False,delta_delta=False):\n        if audio_path == None:\n            raise Exception(\"Path to audio (Wav) file must be provided\")\n        wav,sr = self.readAudio(fpath=audio_path,pre_emp=True)\n        nfft,winlen,winstep = self.getWindow(sr)\n        if(ftype == 'mfcc'):\n            melspec = librosa.feature.mfcc(wav,sr=sr,hop_length=winstep,win_length=winlen,n_fft=nfft,n_mfcc=n_mels)\n            \n        db_melspec = librosa.amplitude_to_db(melspec,ref=np.max)\n        \n        mD = None\n        mDD = None\n        if(delta):\n            mD = librosa.feature.delta(db_melspec)\n            if(delta_delta):\n                mDD = librosa.feature.delta(mD)\n        \n        audio_phon_transcription = None\n        if phon_path == None:\n            tmp = audio_path.split('/')\n            phon_path = \"/\".join(tmp[:(len(tmp)-1)])+\"/\"+ tmp[len(tmp)-1].split('.WAV')[0]+\".PHN\"\n            \n        audio_phon_transcription = self.readPhon(phon_path)            \n        time = db_melspec.shape[1]\n        \n        feature_vectors = []\n        db_melspec = db_melspec.T\n        mD = mD.T\n        mDD = mDD.T\n        \n        prev = None\n        first = audio_phon_transcription.columns\n        audio_phon_transcription.columns = ['start','end','phoneme']\n        labels = []\n        for i in range(time):\n            #---collecting feature---\n            feature = np.zeros(n_mels*3)\n            feature[:n_mels] = db_melspec[i]\n            feature[n_mels:n_mels*2] = mD[i]\n            feature[n_mels*2:n_mels*3] = mDD[i]\n            feature_vectors.append(feature)\n            \n            #---collecting phoneme label ---\n            start = winstep * i\n            end = start+winlen\n            diff = start+400\n            phoneme = list(\n                        audio_phon_transcription[\n                            ((audio_phon_transcription['start']<=start) & \n                            ((audio_phon_transcription['end']-start)>=int(winlen/1.5)))\n                            |\n                            ((audio_phon_transcription['start']<=end) & \n                                (audio_phon_transcription['end']>end))  \n                        ].to_dict()['phoneme'].values()\n            )\n            if len(phoneme) == 0:\n                if int(first[1]) > start:\n                    phoneme = first[2]\n                else:\n                    phoneme = prev\n            else:\n                phoneme = phoneme[0]\n            phoneme = self.get39EquiOf61(phoneme)\n            prev = phoneme\n            labels.append(phoneme)\n             \n        return feature_vectors,labels\n    \n    \n    def prepareLabelsForTraining(self,labels):\n        print('Preparing Labels')\n        label_vector = []\n        p_bar = tqdm(range(len(labels)))\n        c = 0\n        for l in labels:\n            label = [0 for i in range(39)]\n            label[self.label_p39[l]-1] = 1\n            label_vector.append(label)\n            c+=1\n            if c == 500:\n                p_bar.set_description(f'Working on phoneme {l}')\n                p_bar.update(c)\n                c = 0\n           \n        p_bar.set_description(f'Working on phoneme {l}')\n        p_bar.update(c) \n        return label_vector\n    \n    def collectFeatures(self,ft='Train',ftype='mfcc',n_mels=128,delta=False,delta_delta=False):\n        tddA = self.getListAudioFiles(ft)\n        tddA.index = range(tddA.shape[0])\n        feature_vectors = []\n        labels = []\n        \n        p_bar = tqdm(range(tddA.shape[0]))\n        silent_count = 0\n        for i in range(tddA.shape[0]):\n            fv,lv = self.getFeatureAndLabel(ftype=ftype,audio_path=tddA.loc[i][self.f_Path],n_mels=n_mels,delta=delta,delta_delta=delta_delta)\n            p_bar.set_description(f'Working on {tddA.loc[i][self.f_Path]} ,index: {i}  ')\n            p_bar.update()\n            feature_vectors += fv\n            labels += lv\n                   \n        print(f\"length of feature_vectors is {len(feature_vectors)} and length of labels is {len(labels)}\")\n        labels = np.asarray(np.array(self.prepareLabelsForTraining(labels),dtype=object)).astype(np.int16)\n        feature_vectors = np.asarray(np.array(feature_vectors,dtype=object)).astype(np.float32)\n        return feature_vectors,labels        ","metadata":{"execution":{"iopub.status.busy":"2021-12-12T23:15:30.639100Z","iopub.execute_input":"2021-12-12T23:15:30.639637Z","iopub.status.idle":"2021-12-12T23:15:30.711655Z","shell.execute_reply.started":"2021-12-12T23:15:30.639583Z","shell.execute_reply":"2021-12-12T23:15:30.710984Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"####--------------Collecting Training Features----------------------###   \ngc.collect()\ncm = DNN_MODULE_BUILDER(path)\nn_mels = 64\ndelta = True\ndelta_delta=True\nftype = 'mfcc'\n\nprint('Collecting Features from Audio Files')\nfeatures,labels = cm.collectFeatures(ftype=ftype,n_mels=n_mels,delta=delta,delta_delta=delta_delta)\n\nprint('--- Completed')\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-12-12T23:15:36.498716Z","iopub.execute_input":"2021-12-12T23:15:36.499002Z","iopub.status.idle":"2021-12-12T23:52:43.688907Z","shell.execute_reply.started":"2021-12-12T23:15:36.498975Z","shell.execute_reply":"2021-12-12T23:52:43.685199Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting Features from Audio Files\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4620 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"051fbe6ede7c4ed7a6cdd20f146c8d57"}},"metadata":{}},{"name":"stdout","text":"length of feature_vectors is 1421707 and length of labels is 1421707\nPreparing Labels\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1421707 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfe8e086682044d78d9cc436afbd294a"}},"metadata":{}},{"name":"stdout","text":"--- Completed\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"#-------------\nffp = open(\"/kaggle/working/features.pkl\",'wb')\npkl.dump(features,ffp)\nflp = open(\"/kaggle/working/labels.pkl\",'wb')\npkl.dump(labels,flp)            \nffp.close()\nflp.close()","metadata":{"execution":{"iopub.status.busy":"2021-12-12T23:52:58.011354Z","iopub.execute_input":"2021-12-12T23:52:58.011764Z","iopub.status.idle":"2021-12-12T23:53:00.986791Z","shell.execute_reply.started":"2021-12-12T23:52:58.011729Z","shell.execute_reply":"2021-12-12T23:53:00.986000Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(features.shape)\nprint(labels.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T00:12:13.456382Z","iopub.execute_input":"2021-12-13T00:12:13.456718Z","iopub.status.idle":"2021-12-13T00:12:13.461501Z","shell.execute_reply.started":"2021-12-13T00:12:13.456681Z","shell.execute_reply":"2021-12-13T00:12:13.460897Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"(1421707, 192)\n(1421707, 39)\n","output_type":"stream"}]},{"cell_type":"code","source":"####--------------Model Training----------------------###   \nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(units=1024, input_shape=[n_mels*3],activation=tf.nn.relu),\n    tf.keras.layers.Dense(units=1024,activation=tf.nn.relu),\n    tf.keras.layers.Dense(units=1024,activation=tf.nn.relu),\n    tf.keras.layers.Dense(units=39,activation=tf.nn.softmax)\n])\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel.summary()\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T00:12:16.285941Z","iopub.execute_input":"2021-12-13T00:12:16.286369Z","iopub.status.idle":"2021-12-13T00:12:16.688833Z","shell.execute_reply.started":"2021-12-13T00:12:16.286340Z","shell.execute_reply":"2021-12-13T00:12:16.688027Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"2021-12-13 00:12:16.326118: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2021-12-13 00:12:16.365801: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2021-12-13 00:12:16.365862: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n2021-12-13 00:12:16.365947: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (9ccea6c355dc): /proc/driver/nvidia/version does not exist\n2021-12-13 00:12:16.369291: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2021-12-13 00:12:16.370299: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n","output_type":"stream"},{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 1024)              197632    \n_________________________________________________________________\ndense_1 (Dense)              (None, 1024)              1049600   \n_________________________________________________________________\ndense_2 (Dense)              (None, 1024)              1049600   \n_________________________________________________________________\ndense_3 (Dense)              (None, 39)                39975     \n=================================================================\nTotal params: 2,336,807\nTrainable params: 2,336,807\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"history = model.fit(\n    features[:1137000],labels[:1137000],epochs=25,\n     batch_size=512, verbose=1,\n    validation_data=(features[1137000:],labels[1137000:]),\n    validation_batch_size=128,\n)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T00:12:29.567777Z","iopub.execute_input":"2021-12-13T00:12:29.568383Z","iopub.status.idle":"2021-12-13T01:27:55.386531Z","shell.execute_reply.started":"2021-12-13T00:12:29.568334Z","shell.execute_reply":"2021-12-13T01:27:55.385684Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"2021-12-13 00:12:29.577035: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 873216000 exceeds 10% of free system memory.\n2021-12-13 00:12:30.511860: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n2021-12-13 00:12:30.525805: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2199995000 Hz\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/25\n2221/2221 [==============================] - 185s 83ms/step - loss: 3.3222 - accuracy: 0.4000 - val_loss: 1.6835 - val_accuracy: 0.5054\nEpoch 2/25\n2221/2221 [==============================] - 181s 82ms/step - loss: 1.5892 - accuracy: 0.5259 - val_loss: 1.5184 - val_accuracy: 0.5452\nEpoch 3/25\n2221/2221 [==============================] - 181s 81ms/step - loss: 1.4771 - accuracy: 0.5529 - val_loss: 1.5013 - val_accuracy: 0.5493\nEpoch 4/25\n2221/2221 [==============================] - 185s 83ms/step - loss: 1.4100 - accuracy: 0.5702 - val_loss: 1.4778 - val_accuracy: 0.5576\nEpoch 5/25\n2221/2221 [==============================] - 181s 81ms/step - loss: 1.3619 - accuracy: 0.5810 - val_loss: 1.4278 - val_accuracy: 0.5717\nEpoch 6/25\n2221/2221 [==============================] - 181s 82ms/step - loss: 1.3313 - accuracy: 0.5895 - val_loss: 1.4402 - val_accuracy: 0.5667\nEpoch 7/25\n2221/2221 [==============================] - 181s 82ms/step - loss: 1.3038 - accuracy: 0.5966 - val_loss: 1.4013 - val_accuracy: 0.5760\nEpoch 8/25\n2221/2221 [==============================] - 182s 82ms/step - loss: 1.2818 - accuracy: 0.6021 - val_loss: 1.3831 - val_accuracy: 0.5824\nEpoch 9/25\n2221/2221 [==============================] - 182s 82ms/step - loss: 1.2649 - accuracy: 0.6062 - val_loss: 1.4194 - val_accuracy: 0.5750\nEpoch 10/25\n2221/2221 [==============================] - 181s 82ms/step - loss: 1.2491 - accuracy: 0.6112 - val_loss: 1.4014 - val_accuracy: 0.5781\nEpoch 11/25\n2221/2221 [==============================] - 180s 81ms/step - loss: 1.2354 - accuracy: 0.6141 - val_loss: 1.4011 - val_accuracy: 0.5760\nEpoch 12/25\n2221/2221 [==============================] - 180s 81ms/step - loss: 1.2207 - accuracy: 0.6178 - val_loss: 1.3910 - val_accuracy: 0.5802\nEpoch 13/25\n2221/2221 [==============================] - 179s 80ms/step - loss: 1.2121 - accuracy: 0.6207 - val_loss: 1.3862 - val_accuracy: 0.5828\nEpoch 14/25\n2221/2221 [==============================] - 181s 81ms/step - loss: 1.2021 - accuracy: 0.6227 - val_loss: 1.4056 - val_accuracy: 0.5819\nEpoch 15/25\n2221/2221 [==============================] - 181s 81ms/step - loss: 1.1955 - accuracy: 0.6247 - val_loss: 1.3988 - val_accuracy: 0.5843\nEpoch 16/25\n2221/2221 [==============================] - 180s 81ms/step - loss: 1.1826 - accuracy: 0.6283 - val_loss: 1.3779 - val_accuracy: 0.5861\nEpoch 17/25\n2221/2221 [==============================] - 179s 81ms/step - loss: 1.1798 - accuracy: 0.6297 - val_loss: 1.3964 - val_accuracy: 0.5843\nEpoch 18/25\n2221/2221 [==============================] - 179s 81ms/step - loss: 1.1697 - accuracy: 0.6321 - val_loss: 1.3982 - val_accuracy: 0.5827\nEpoch 19/25\n2221/2221 [==============================] - 181s 81ms/step - loss: 1.1666 - accuracy: 0.6331 - val_loss: 1.3991 - val_accuracy: 0.5824\nEpoch 20/25\n2221/2221 [==============================] - 179s 81ms/step - loss: 1.1581 - accuracy: 0.6354 - val_loss: 1.4002 - val_accuracy: 0.5806\nEpoch 21/25\n2221/2221 [==============================] - 180s 81ms/step - loss: 1.1536 - accuracy: 0.6361 - val_loss: 1.4163 - val_accuracy: 0.5806\nEpoch 22/25\n2221/2221 [==============================] - 181s 81ms/step - loss: 1.1488 - accuracy: 0.6378 - val_loss: 1.4141 - val_accuracy: 0.5802\nEpoch 23/25\n2221/2221 [==============================] - 183s 82ms/step - loss: 1.1436 - accuracy: 0.6393 - val_loss: 1.4289 - val_accuracy: 0.5813\nEpoch 24/25\n2221/2221 [==============================] - 182s 82ms/step - loss: 1.1402 - accuracy: 0.6399 - val_loss: 1.4009 - val_accuracy: 0.5857\nEpoch 25/25\n2221/2221 [==============================] - 182s 82ms/step - loss: 1.1348 - accuracy: 0.6412 - val_loss: 1.3995 - val_accuracy: 0.5829\n","output_type":"stream"}]},{"cell_type":"code","source":"###------------collecting test features -------------------\ngc.collect()\ntest_features,test_labels = cm.collectFeatures(ft='Test',ftype=ftype,n_mels=n_mels,delta=delta,delta_delta=delta_delta)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-12-12T23:54:50.224035Z","iopub.execute_input":"2021-12-12T23:54:50.224328Z","iopub.status.idle":"2021-12-13T00:08:52.277307Z","shell.execute_reply.started":"2021-12-12T23:54:50.224300Z","shell.execute_reply":"2021-12-13T00:08:52.276354Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1680 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e8c0f0e20a140d69fcba307f7a4601e"}},"metadata":{}},{"name":"stdout","text":"length of feature_vectors is 519525 and length of labels is 519525\nPreparing Labels\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/519525 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d156e9ecac5449595e480f7d106963f"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"####--------------Model Evaluating----------------------###   \nevaluation = model.evaluate(test_features,test_labels,batch_size=128)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:27:55.388661Z","iopub.execute_input":"2021-12-13T01:27:55.388919Z","iopub.status.idle":"2021-12-13T01:28:29.218237Z","shell.execute_reply.started":"2021-12-13T01:27:55.388890Z","shell.execute_reply":"2021-12-13T01:28:29.217551Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"4059/4059 [==============================] - 33s 8ms/step - loss: 1.4050 - accuracy: 0.5806\n","output_type":"stream"}]}]}